---
title: "Entrada 09 ‚Äî Redes Neuronales: del Perceptr√≥n al MLP"
date: 2025-10-10
---

# Entrada 09 ‚Äî Redes Neuronales: del Perceptr√≥n al MLP

## Contexto
En esta pr√°ctica analizamos c√≥mo las redes neuronales evolucionan desde el perceptr√≥n simple hasta el modelo multicapa (MLP), explorando su capacidad para resolver problemas no lineales.  
A trav√©s de ejemplos como las compuertas l√≥gicas AND, OR, NOT y XOR, se comprob√≥ emp√≠ricamente qu√© casos puede o no resolver un perceptr√≥n, y por qu√© el MLP logra superar esas limitaciones.  
Tambi√©n se abordaron comparaciones entre frameworks como Scikit-learn, TensorFlow, y PyTorch Lightning, entendiendo las ventajas, limitaciones y enfoques de cada uno.

## Objetivos
- Comprender por qu√© un perceptr√≥n simple no puede resolver XOR.  
- Analizar el rol del bias y los pesos en compuertas l√≥gicas AND y OR.  
- Aplicar un MLP (Multi-Layer Perceptron) con Scikit-learn para resolver XOR.  
- Diferenciar los enfoques de entrenamiento entre Scikit-learn, TensorFlow y PyTorch.  
- Identificar ventajas, desventajas y casos de uso de cada framework.  

## Actividades (con tiempos estimados)
- Revisi√≥n del c√≥digo del perceptr√≥n y an√°lisis de la frontera de decisi√≥n ‚Äî 20 min  
- Implementaci√≥n de compuertas AND, OR, NOT y XOR ‚Äî 25 min  
- Entrenamiento de un modelo MLP en Scikit-learn para resolver XOR ‚Äî 30 min  
- Comparaci√≥n conceptual de frameworks (TensorFlow, PyTorch, Lightning) ‚Äî 30 min  
- Resoluci√≥n de cuestionario te√≥rico con 14 preguntas ‚Äî 40 min  

## Desarrollo
Durante esta pr√°ctica se probaron distintos casos l√≥gicos con el perceptr√≥n b√°sico, comprobando que solo los linealmente separables (AND, OR, NOT) pueden resolverse con una √∫nica frontera de decisi√≥n.  
Luego se implement√≥ una red neuronal multicapa con Scikit-learn (MLPClassifier) y se verific√≥ que el problema XOR, que no puede dividirse con una l√≠nea recta, s√≠ puede resolverse mediante capas ocultas.  
Finalmente, se compararon las principales bibliotecas de machine learning, entendiendo c√≥mo cada una gestiona el proceso de entrenamiento, optimizaci√≥n y control de par√°metros.

---

### Preguntas y respuestas

**1Ô∏è‚É£ ¬øPor qu√© AND, OR y NOT funcionaron pero XOR no?**  
Porque XOR no es linealmente separable, no puede dividirse con una sola l√≠nea recta. El perceptr√≥n simple solo puede crear una frontera lineal.

---

**2Ô∏è‚É£ ¬øCu√°l es la diferencia clave entre los pesos de AND vs OR?**  
El umbral (bias). En AND, el umbral debe ser m√°s alto; en OR, m√°s bajo. En otras palabras, OR se activa m√°s f√°cilmente que AND.

---

**3Ô∏è‚É£ ¬øQu√© otros problemas del mundo real ser√≠an como XOR?**  
Situaciones donde solo una de dos condiciones puede cumplirse:  
- Un sem√°foro que no puede estar en rojo y verde al mismo tiempo.  
- Un sistema de alarma que se activa por una condici√≥n u otra, pero no ambas.

---

**4Ô∏è‚É£ ¬øPor qu√© sklearn MLP puede resolver XOR pero un perceptr√≥n no?**  
Porque el MLP tiene capas ocultas que generan m√∫ltiples l√≠neas de decisi√≥n y pueden combinarse para crear divisiones no lineales.

---

**5Ô∏è‚É£ ¬øCu√°l es la principal diferencia entre TensorFlow/Keras y sklearn MLP?**  
TensorFlow/Keras permite controlar cada detalle del entrenamiento, mientras que Scikit-learn simplifica el proceso, ideal para prototipos r√°pidos.

---

**6Ô∏è‚É£ ¬øPor qu√© TensorFlow usa epochs y batch_size mientras sklearn MLP no?**  
Porque TensorFlow entrena en lotes (batches) y repite los pasos (epochs) para optimizar de forma flexible.  
Scikit-learn entrena todo el conjunto de datos de una sola vez.

---

**7Ô∏è‚É£ ¬øCu√°ndo usar√≠as sigmoid vs relu como funci√≥n de activaci√≥n?**  
- *Sigmoid:* en la capa de salida, para clasificaciones binarias.  
- *ReLU:* en capas ocultas, para mejorar el rendimiento y evitar gradientes nulos.

---

**8Ô∏è‚É£ ¬øQu√© ventaja tiene PyTorch Lightning sobre TensorFlow puro?**  
Reduce el c√≥digo repetitivo, separa mejor las etapas del entrenamiento y facilita la experimentaci√≥n.

---

**9Ô∏è‚É£ ¬øPor qu√© PyTorch Lightning separa training_step y test_step?**  
Porque en el entrenamiento se actualizan los pesos, mientras que en la evaluaci√≥n solo se calculan m√©tricas sin modificar el modelo.

---

**üîü ¬øQu√© framework elegir√≠as para cada escenario?**  
- Prototipo r√°pido: **Scikit-learn**  
- Modelo en producci√≥n: **TensorFlow/Keras**  
- Investigaci√≥n avanzada: **PyTorch/PyTorch Lightning**

---

**11Ô∏è‚É£ ¬øPor qu√© el error ‚Äúmat1 and mat2 shapes cannot be multiplied‚Äù es com√∫n en PyTorch?**  
Porque las dimensiones del dataset no coinciden con la cantidad de neuronas de entrada del modelo.

---

**12Ô∏è‚É£ ¬øQu√© significa deterministic=True en PyTorch Lightning Trainer?**  
Fuerza la reproducibilidad de resultados entre ejecuciones, fijando las semillas aleatorias.

---

**13Ô∏è‚É£ ¬øPor qu√© TensorFlow muestra curvas de loss y val_loss durante entrenamiento?**  
Para visualizar posibles casos de overfitting: si la p√©rdida de validaci√≥n aumenta mientras la de entrenamiento baja, el modelo est√° sobreajustando.

---

**14Ô∏è‚É£ ¬øCu√°l es la diferencia entre trainer.test() y trainer.predict() en PyTorch Lightning?**  
- `trainer.test()` calcula m√©tricas de rendimiento.  
- `trainer.predict()` genera predicciones sin medir desempe√±o.

---

**15Ô∏è‚É£ ¬øPor qu√© sklearn MLP es m√°s f√°cil pero menos flexible?**  
Porque automatiza muchos pasos del proceso de entrenamiento, sacrificando personalizaci√≥n y control detallado del modelo.

---

## Evidencias
https://colab.research.google.com/drive/1CVYq5nakYt-6JPhYoUuahuVJZd2I4u-9?usp=sharing

## Reflexi√≥n
En esta pr√°ctica entend√≠ la diferencia entre problemas lineales y no lineales, y por qu√© una red multicapa permite resolver los segundos.  
Tambi√©n comprend√≠ c√≥mo el bias y los pesos definen la frontera de decisi√≥n, y c√≥mo las funciones de activaci√≥n cambian el comportamiento de las neuronas.  
Finalmente, aprend√≠ que la elecci√≥n del framework depende del objetivo: rapidez, producci√≥n o investigaci√≥n.

## Pr√≥ximos pasos
Proceder√© a realizar tarea 2 de la UT2
